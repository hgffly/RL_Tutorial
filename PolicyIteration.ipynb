{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, height, width, gamma=1, terminals=[(0, 0)], maze=None):\n",
    "        self.__actions = (\"U\", \"D\", \"L\", \"R\")\n",
    "        self.__reward = -1\n",
    "        \n",
    "        self.__gamma = gamma\n",
    "        \n",
    "        if maze is None:\n",
    "            self.__maze = None\n",
    "            self.__width = width\n",
    "            self.__height = height\n",
    "    \n",
    "            self.__terminals = terminals\n",
    "        \n",
    "            self.__states = []\n",
    "            for h in range(height):\n",
    "                for w in range(width):\n",
    "                    self.__states += [(h, w)]\n",
    "        else:\n",
    "            self.__maze = maze\n",
    "            self.__width = len(maze[0])\n",
    "            self.__height = len(maze)\n",
    "\n",
    "            self.__terminals = []        \n",
    "            self.__states = []\n",
    "            for h in range(self.__height):\n",
    "                for w in range(self.__width):\n",
    "                    if maze[h][w] == \"O\":\n",
    "                        self.__states += [(h, w)]\n",
    "                    elif maze[h][w] == \"T\":\n",
    "                        self.__states += [(h, w)]\n",
    "                        self.__terminals += [(h, w)]                    \n",
    "\n",
    "    def try_action(self, state, action):\n",
    "        if state in self.__terminals:\n",
    "            return [], [], []\n",
    "        \n",
    "        pos_y, pos_x = state\n",
    "        if action == \"U\":\n",
    "            pos_y = max(0, pos_y-1)\n",
    "        elif action == \"D\":\n",
    "            pos_y = min(self.height-1, pos_y+1)\n",
    "        elif action == \"L\":\n",
    "            pos_x = max(0, pos_x-1)\n",
    "        elif action == \"R\":\n",
    "            pos_x = min(self.width-1, pos_x+1)\n",
    "           \n",
    "        if self.__maze is not None and self.__maze[pos_y][pos_x] == \"X\":\n",
    "            return [state], [1], [self.__reward]\n",
    "\n",
    "        return [(pos_y, pos_x)], [1], [self.__reward]  # next_states, transition prob, reward\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.__height\n",
    "    \n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.__width\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self.__gamma\n",
    "    \n",
    "    @property\n",
    "    def actions(self):\n",
    "        return self.__actions\n",
    "    \n",
    "    @property\n",
    "    def states(self):\n",
    "        return self.__states\n",
    "    \n",
    "    @property\n",
    "    def terminals(self):\n",
    "        return self.__terminals\n",
    "\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, env, theta):\n",
    "        self._env = env\n",
    "        \n",
    "        self._state_values = {s:0 for s in env.states}\n",
    "        \n",
    "        self._theta = theta\n",
    "        \n",
    "    def print_state_values(self):\n",
    "        print(\"state values\")\n",
    "        for h in range(self._env.height):\n",
    "            sv = []\n",
    "            for w in range(self._env.width):\n",
    "                if (h, w) in self._env.states:\n",
    "                    sv += [f\"{self._state_values[(h, w)]:+1.1f}\"]\n",
    "                else:\n",
    "                    sv += [\"-INF\"]\n",
    "            print(sv)\n",
    "            \n",
    "    def print_policy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "            \n",
    "class PolicyIteration(Agent):\n",
    "    def __init__(self, env, theta=2):\n",
    "        super().__init__(env, theta)\n",
    "\n",
    "        self.set_random_policy()\n",
    "        \n",
    "    def set_random_policy(self):\n",
    "        self._policy = {s:self._env.actions for s in self._env.states}\n",
    "        \n",
    "        self.set_terminals(self._env.terminals)\n",
    "        \n",
    "        self.cal_action_prob()\n",
    "    \n",
    "    def set_terminals(self, terminals):\n",
    "        for t in terminals:\n",
    "            self._policy[t] = []\n",
    "    \n",
    "    def cal_action_prob(self):\n",
    "        self._action_prob = {}\n",
    "        for s in self._env.states:\n",
    "            self._action_prob[s] = {}\n",
    "            for a in self._policy[s]:\n",
    "                self._action_prob[s][a] = 1 / len(self._policy[s])\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        for i in range(10):  # if theta is too small so that it can't converge then stop after 10 iterations\n",
    "            delta = 0\n",
    "            tmp_state_values = copy.deepcopy(self._state_values)\n",
    "            for s in self._env.states:\n",
    "                action_value = 0\n",
    "                action_prob = 0\n",
    "                for a in self._policy[s]:  # try action from the current policy\n",
    "                    next_states, transition_probs, rewards = self._env.try_action(s, a)\n",
    "                    action_prob = self._action_prob[s][a]\n",
    "                    for next_state, trans_prob, reward in zip(next_states, transition_probs, rewards):\n",
    "                        action_value += trans_prob * (reward + self._env.gamma * self._state_values[next_state])\n",
    "\n",
    "                tmp_state_values[s] = action_prob * action_value\n",
    "                delta = max(delta, abs(tmp_state_values[s]-self._state_values[s]))\n",
    "                \n",
    "            self._state_values = copy.deepcopy(tmp_state_values)\n",
    "            \n",
    "            if delta < self._theta:\n",
    "                break\n",
    "        \n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        for s in self._env.states:\n",
    "            action_values = {}\n",
    "            for a in self._env.actions:\n",
    "                if a in self._policy[s]:\n",
    "                    next_states, transition_probs, rewards = self._env.try_action(s, a)\n",
    "                    action_prob = self._action_prob[s][a]\n",
    "                    for next_state, trans_prob, reward in zip(next_states, transition_probs, rewards):\n",
    "                        action_values[a] = trans_prob * (reward + self._env.gamma * self._state_values[next_state])\n",
    "                else:\n",
    "                    action_values[a] = -np.inf\n",
    "               \n",
    "            old_policy = self._policy[s]\n",
    "            max_action_value = max(action_values.values())\n",
    "            if max_action_value != -np.inf:\n",
    "                self._policy[s] = tuple(a for a in self._env.actions if action_values[a]==max_action_value)  # select actions achieving max action value\n",
    "                if old_policy != self._policy[s]:\n",
    "                    policy_stable = False\n",
    "                    \n",
    "        self.cal_action_prob()\n",
    "        \n",
    "        return policy_stable    \n",
    "     \n",
    "    def perform(self, iter_num, random_policy=False):\n",
    "        print(\"initial\")\n",
    "        self.print_policy()\n",
    "        self.print_state_values()\n",
    "            \n",
    "        for i in range(iter_num):\n",
    "            print(\"\\nstep: \", i+1)\n",
    "            self.policy_evaluation()\n",
    "\n",
    "            self.print_state_values()\n",
    "\n",
    "            policy_stable = self.policy_improvement()\n",
    "            self.print_policy()\n",
    "            \n",
    "            #self.print_action_prob()\n",
    "            if random_policy is True:\n",
    "                self.set_random_policy()\n",
    "            \n",
    "            if policy_stable is True:\n",
    "                break\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(\"policy\")\n",
    "        for h in range(self._env.height):\n",
    "            ss = []\n",
    "            for w in range(self._env.width):\n",
    "                if (h, w) in self._env.states:\n",
    "                    action = \"\".join([x if x in self._policy[(h, w)] else \"_\" for x in self._env.actions])\n",
    "                else:\n",
    "                    action = \"XXXX\"\n",
    "\n",
    "                ss += [action]\n",
    "            print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 4, 4\n",
    "maze = [\"OOOOXXX\", \n",
    "        \"OXOXOTX\",\n",
    "        \"OXOXOOX\",\n",
    "        \"OOOOOOO\",]\n",
    "env = GridWorld(h, w, terminals=((0, 0), (h-1, w-1)))\n",
    "#env = GridWorld(h, w, terminals=((h>>1, w>>1), ))\n",
    "#env = GridWorld(h, w, terminals=((0, 0), (h-1, w-1)), maze=maze)\n",
    "policy_iteration = PolicyIteration(env)\n",
    "policy_iteration.perform(30, random_policy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration(Agent):\n",
    "    def __init__(self, env, theta=0.1):\n",
    "        super().__init__(env, theta)\n",
    "        \n",
    "        self.init_policy()\n",
    "        \n",
    "    def init_policy(self):\n",
    "        self._policy = {s:self._env.actions[0] for s in self._env.states}  # take up action as the default\n",
    "         \n",
    "        for t in self._env.terminals:\n",
    "            print(t)\n",
    "            self._policy[t] = \"_\"\n",
    "    \n",
    "    def optimize(self, max_iter=30):\n",
    "        for i in range(max_iter):  # if theta is too small so that it can't converge then stop after max_iter iterations\n",
    "            delta = 0\n",
    "            tmp_state_values = copy.deepcopy(self._state_values)\n",
    "            for s in self._env.states:\n",
    "                if s in self._env.terminals:\n",
    "                    continue\n",
    "                    \n",
    "                action_values = []\n",
    "                for a in self._env.actions:\n",
    "                    next_states, transition_probs, rewards = self._env.try_action(s, a)                        \n",
    "                    for next_state, trans_prob, reward in zip(next_states, transition_probs, rewards):\n",
    "                        action_values += [trans_prob * (reward + self._env.gamma * self._state_values[next_state])]\n",
    "\n",
    "                tmp_state_values[s], self._policy[s] = max(action_values), self._env.actions[np.argmax(action_values)]\n",
    "                delta = max(delta, abs(tmp_state_values[s]-self._state_values[s]))                \n",
    "              \n",
    "            print(\"\\nstep: \", i+1)\n",
    "            self._state_values = copy.deepcopy(tmp_state_values)\n",
    "            \n",
    "            self.print_state_values()\n",
    "            \n",
    "            if delta < self._theta:\n",
    "                break\n",
    "     \n",
    "    def perform(self):\n",
    "        print(\"initial\")\n",
    "        self.print_state_values()\n",
    "        self.print_policy()\n",
    "\n",
    "        self.optimize()\n",
    "\n",
    "        self.print_policy()\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(\"policy\")\n",
    "        for h in range(self._env.height):\n",
    "            ss = []\n",
    "            for w in range(self._env.width):\n",
    "                ss += [self._policy[(h, w)]]\n",
    "            print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 5, 5\n",
    "env = GridWorld(h, w, terminals=((0, 0), (h-1, w-1)))\n",
    "#env = GridWorld(h, w, terminals=((h>>1, w>>1), ))\n",
    "\n",
    "policy_iteration = ValueIteration(env)\n",
    "policy_iteration.perform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
