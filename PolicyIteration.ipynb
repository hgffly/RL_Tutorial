{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, height, width, gamma=1, terminals=[(0, 0)]):\n",
    "        self.actions = (\"U\", \"D\", \"L\", \"R\")\n",
    "        self.reward = -1\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.states = []\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                self.states += [(h, w)]\n",
    "        self.state_values = {s:0 for s in self.states}\n",
    "        self.gamma = gamma\n",
    "        self.terminals = terminals\n",
    "        self.set_random_policy()\n",
    "        \n",
    "    def set_random_policy(self):\n",
    "        self.policy = {s:self.actions for s in self.states}\n",
    "        \n",
    "        self.set_terminals(self.terminals)\n",
    "        \n",
    "        self.cal_action_prob()\n",
    "    \n",
    "    def set_terminals(self, terminals):\n",
    "        for t in terminals:\n",
    "            self.policy[t] = []\n",
    "    \n",
    "    def cal_action_prob(self):\n",
    "        self.action_prob = {}\n",
    "        for s in self.states:\n",
    "            self.action_prob[s] = {}\n",
    "            for a in self.policy[s]:\n",
    "                self.action_prob[s][a] = 1 / len(self.policy[s])\n",
    "\n",
    "    def try_action(self, state, action):\n",
    "        if state in self.terminals:\n",
    "            return [], []\n",
    "        pos_y, pos_x = state\n",
    "        if action == \"U\":\n",
    "            if pos_y-1 >= 0:\n",
    "                pos_y -= 1\n",
    "        elif action == \"D\":\n",
    "            if pos_y+1 < self.height:\n",
    "                pos_y += 1\n",
    "        elif action == \"L\":\n",
    "            if pos_x-1 >= 0:\n",
    "                pos_x -= 1\n",
    "        elif action == \"R\":\n",
    "            if pos_x+1 < self.width:\n",
    "                pos_x += 1\n",
    "            \n",
    "        return [(pos_y, pos_x)], [1]  # next_state, transition prob\n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        theta = 2\n",
    "        delta = theta\n",
    "        \n",
    "        idx = 1\n",
    "        while delta >= theta:\n",
    "            delta = 0\n",
    "            tmp_state_values = copy.deepcopy(self.state_values)\n",
    "            for s in self.states:\n",
    "                action_value = 0\n",
    "                action_prob = 0\n",
    "                for a in self.policy[s]:  # try action from the current policy\n",
    "                    next_states, transition_probs = self.try_action(s, a)                        \n",
    "                    action_prob = self.action_prob[s][a]\n",
    "                    for next_state, trans_prob in zip(next_states, transition_probs):\n",
    "                        action_value += trans_prob * (self.reward + self.gamma * self.state_values[next_state])\n",
    "\n",
    "                tmp_state_values[s] = action_prob * action_value\n",
    "                delta = max(delta, abs(tmp_state_values[s]-self.state_values[s]))\n",
    "                \n",
    "            self.state_values = copy.deepcopy(tmp_state_values)\n",
    "            idx += 1\n",
    "            if idx > 10:  # if theta is too small so that it can't converge then stop after 10 iterations\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        for s in self.states:\n",
    "            action_values = {}\n",
    "            for a in self.actions:\n",
    "                if a in self.policy[s]:\n",
    "                    next_states, transition_probs = self.try_action(s, a)\n",
    "                    action_prob = self.action_prob[s][a]\n",
    "                    for next_state, trans_prob in zip(next_states, transition_probs):\n",
    "                        action_values[a] = trans_prob * (self.reward + self.gamma * self.state_values[next_state])\n",
    "                else:\n",
    "                    action_values[a] = -np.inf\n",
    "               \n",
    "            old_policy = self.policy[s]\n",
    "            max_action_value = max(action_values.values())\n",
    "            if max_action_value != -np.inf:\n",
    "                self.policy[s] = tuple(a for a in self.actions if action_values[a]==max_action_value)\n",
    "                if old_policy != self.policy[s]:\n",
    "                    policy_stable = False\n",
    "                    \n",
    "        self.cal_action_prob()\n",
    "        \n",
    "        return policy_stable\n",
    "            \n",
    "     \n",
    "    def perform(self, iter_num, random_policy=False):\n",
    "        print(\"initial\")\n",
    "        self.print_policy()\n",
    "        self.print_state_values()\n",
    "            \n",
    "        for i in range(iter_num):\n",
    "            print(\"\\nstep: \", i+1)\n",
    "            self.policy_evaluation()\n",
    "\n",
    "            self.print_state_values()\n",
    "\n",
    "            policy_stable = self.policy_improvement()\n",
    "            self.print_policy()\n",
    "            \n",
    "            #self.print_action_prob()\n",
    "            if random_policy is True:\n",
    "                self.set_random_policy()\n",
    "            \n",
    "            if policy_stable is True:\n",
    "                break\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(\"policy\")\n",
    "        for h in range(self.height):\n",
    "            ss = []\n",
    "            for w in range(self.width):\n",
    "                action = \"\".join([x if x in self.policy[(h, w)] else \"_\" for x in self.actions])\n",
    "\n",
    "                ss += [action]\n",
    "            print(ss)\n",
    "            \n",
    "    def print_action_prob(self):\n",
    "        print(\"action_prob: \", self.action_prob)\n",
    "        \n",
    "    def print_state_values(self):\n",
    "        print(\"state values\")\n",
    "        for h in range(self.height):\n",
    "            sv = []\n",
    "            for w in range(self.width):\n",
    "                sv += [f\"{self.state_values[(h, w)]:+1.1f}\"]\n",
    "            print(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 4, 4\n",
    "policy_iteration = PolicyIteration(h, w, terminals=((0, 0), (h-1, w-1)))\n",
    "\n",
    "policy_iteration.perform(30, random_policy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration(PolicyIteration):\n",
    "    def __init__(self, height, width, gamma=1, terminals=[(0, 0)]):\n",
    "        super().__init__(height, width, gamma, terminals)\n",
    "        \n",
    "        self.policy = {s:self.actions[0] for s in self.states}\n",
    "         \n",
    "        for t in terminals:\n",
    "            self.policy[t] = \"_\"\n",
    "        \n",
    "    \n",
    "    def optimize(self):\n",
    "        theta = 0.1\n",
    "        delta = 10\n",
    "        \n",
    "        idx = 1\n",
    "        while delta > theta:\n",
    "            delta = 0\n",
    "            tmp_state_values = copy.deepcopy(self.state_values)\n",
    "            for s in self.states:\n",
    "                if s in self.terminals:\n",
    "                    continue\n",
    "                    \n",
    "                action_values = []\n",
    "                for a in self.actions:\n",
    "                    next_states, transition_probs = self.try_action(s, a)                        \n",
    "                    for next_state, trans_prob in zip(next_states, transition_probs):\n",
    "                        action_values += [trans_prob * (self.reward + self.gamma * self.state_values[next_state])]\n",
    "\n",
    "                tmp_state_values[s], self.policy[s] = max(action_values), self.actions[np.argmax(action_values)]\n",
    "                delta = max(delta, abs(tmp_state_values[s]-self.state_values[s]))                \n",
    "              \n",
    "            print(\"\\nstep: \", idx)\n",
    "            self.state_values = copy.deepcopy(tmp_state_values)\n",
    "            \n",
    "            self.print_state_values()\n",
    "            idx += 1\n",
    "            if idx > 10:\n",
    "                break           \n",
    "     \n",
    "    def perform(self):\n",
    "        print(\"initial\")\n",
    "        self.print_state_values()\n",
    "        self.print_policy()\n",
    "\n",
    "        self.optimize()\n",
    "\n",
    "        self.print_policy()\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(\"policy\")\n",
    "        for h in range(self.height):\n",
    "            ss = []\n",
    "            for w in range(self.width):\n",
    "                ss += [self.policy[(h, w)]]\n",
    "            print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 4, 4\n",
    "policy_iteration = ValueIteration(h, w, terminals=((0, 0), (h-1, w-1)), gamma=1)\n",
    "\n",
    "policy_iteration.perform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
