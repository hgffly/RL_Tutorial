{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, height, width, gamma=1, terminals=[(0, 0)]):\n",
    "        self.actions = (\"U\", \"D\", \"L\", \"R\")\n",
    "        self.reward = -1\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.states = []\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                self.states += [(h, w)]\n",
    "        self.state_values = {s:0 for s in self.states}\n",
    "        self.gamma = gamma\n",
    "        self.policy = {s:self.actions for s in self.states}\n",
    "        \n",
    "        \n",
    "        self.set_terminals(terminals)\n",
    "        \n",
    "        self.cal_action_prob()\n",
    "    \n",
    "    \n",
    "    def set_terminals(self, terminals):\n",
    "        for t in terminals:\n",
    "            self.policy[t] = []\n",
    "    \n",
    "    def cal_action_prob(self):\n",
    "        self.action_prob = {}\n",
    "        for s in self.states:\n",
    "            self.action_prob[s] = {}\n",
    "            for a in self.policy[s]:\n",
    "                self.action_prob[s][a] = 1 / len(self.policy[s])\n",
    "\n",
    "    def try_action(self, state, action):\n",
    "        pos_y, pos_x = state\n",
    "        if action == \"U\":\n",
    "            if pos_y-1 >= 0:\n",
    "                pos_y -= 1\n",
    "        elif action == \"D\":\n",
    "            if pos_y+1 < self.height:\n",
    "                pos_y += 1\n",
    "        elif action == \"L\":\n",
    "            if pos_x-1 >= 0:\n",
    "                pos_x -= 1\n",
    "        elif action == \"R\":\n",
    "            if pos_x+1 < self.width:\n",
    "                pos_x += 1\n",
    "            \n",
    "        return self.action_prob[state][action], [(pos_y, pos_x)], [1]\n",
    "    \n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        theta = 1\n",
    "        delta = 10\n",
    "        \n",
    "        idx = 0\n",
    "        pre_state_values = copy.deepcopy(self.state_values)\n",
    "        while delta > theta:\n",
    "            delta = 0\n",
    "            tmp_state_values = copy.deepcopy(pre_state_values)\n",
    "            for s in self.states:\n",
    "                action_value = 0\n",
    "                action_prob = 0\n",
    "                for a in self.policy[s]:\n",
    "                    action_prob, next_states, transition_probs = self.try_action(s, a)                        \n",
    "                    for next_state, trans_prob in zip(next_states, transition_probs):\n",
    "                        action_value += trans_prob * (self.reward + self.gamma * pre_state_values[next_state])\n",
    "                \n",
    "                tmp_state_values[s] = action_prob * action_value\n",
    "                delta = max(delta, abs(tmp_state_values[s]-pre_state_values[s]))\n",
    "                \n",
    "            pre_state_values = copy.deepcopy(tmp_state_values)\n",
    "            idx += 1\n",
    "            if idx > 10:\n",
    "                break\n",
    "        self.state_values = copy.deepcopy(tmp_state_values)\n",
    "        \n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        for s in self.states:\n",
    "            action_values = {}\n",
    "            for a in self.actions:\n",
    "                if a in self.policy[s]:\n",
    "                    action_prob, next_states, transition_probs = self.try_action(s, a)\n",
    "                    for next_state, trans_prob in zip(next_states, transition_probs):\n",
    "                        action_values[a] = trans_prob * (self.reward + self.gamma * self.state_values[next_state])\n",
    "                else:\n",
    "                    action_values[a] = -np.inf\n",
    "               \n",
    "            old_policy = self.policy[s]\n",
    "            max_action_value = max(action_values.values())\n",
    "            if max_action_value != -np.inf:\n",
    "                self.policy[s] = [a for a in self.actions if action_values[a]==max_action_value]\n",
    "                if old_policy != self.policy[s]:\n",
    "                    policy_stable = False\n",
    "                    \n",
    "        self.cal_action_prob()\n",
    "        \n",
    "        return policy_stable\n",
    "            \n",
    "     \n",
    "    def perform(self, iter_num):\n",
    "        print(\"initial\")\n",
    "        policy_iteration.print_policy()\n",
    "        policy_iteration.print_state_values()\n",
    "            \n",
    "        for i in range(iter_num):\n",
    "            print(\"\\nstep: \", i+1)\n",
    "            self.policy_evaluation()\n",
    "\n",
    "            self.print_state_values()\n",
    "\n",
    "            policy_stable = self.policy_improvement()\n",
    "            self.print_policy()\n",
    "            \n",
    "            #self.print_action_prob()\n",
    "            \n",
    "            if policy_stable == True:\n",
    "                break\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(\"policy\")\n",
    "        for h in range(self.height):\n",
    "            ss = []\n",
    "            for w in range(self.width):\n",
    "                action = \"\".join([x if x in self.policy[(h, w)] else \"_\" for x in self.actions])\n",
    "\n",
    "                ss += [action]\n",
    "            print(ss)\n",
    "            \n",
    "    def print_action_prob(self):\n",
    "        print(\"action_prob: \", self.action_prob)\n",
    "        \n",
    "    def print_state_values(self):\n",
    "        print(\"state values\")\n",
    "        for h in range(self.height):\n",
    "            sv = []\n",
    "            for w in range(self.width):\n",
    "                sv += [f\"{self.state_values[(h, w)]:+1.1f}\"]\n",
    "            print(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 4, 4\n",
    "policy_iteration = PolicyIteration(h, w, terminals=((0, 0), (h-1, w-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial\n",
      "policy\n",
      "['____', 'UDLR', 'UDLR', 'UDLR']\n",
      "['UDLR', 'UDLR', 'UDLR', 'UDLR']\n",
      "['UDLR', 'UDLR', 'UDLR', 'UDLR']\n",
      "['UDLR', 'UDLR', 'UDLR', '____']\n",
      "state values\n",
      "['+0.0', '+0.0', '+0.0', '+0.0']\n",
      "['+0.0', '+0.0', '+0.0', '+0.0']\n",
      "['+0.0', '+0.0', '+0.0', '+0.0']\n",
      "['+0.0', '+0.0', '+0.0', '+0.0']\n",
      "\n",
      "step:  1\n",
      "state values\n",
      "['+0.0', '-1.0', '-1.0', '-1.0']\n",
      "['-1.0', '-1.0', '-1.0', '-1.0']\n",
      "['-1.0', '-1.0', '-1.0', '-1.0']\n",
      "['-1.0', '-1.0', '-1.0', '+0.0']\n",
      "policy\n",
      "['____', '__L_', 'UDLR', 'UDLR']\n",
      "['U___', 'UDLR', 'UDLR', 'UDLR']\n",
      "['UDLR', 'UDLR', 'UDLR', '_D__']\n",
      "['UDLR', 'UDLR', '___R', '____']\n",
      "\n",
      "step:  2\n",
      "state values\n",
      "['+0.0', '-1.0', '-2.0', '-2.0']\n",
      "['-1.0', '-2.0', '-2.0', '-2.0']\n",
      "['-2.0', '-2.0', '-2.0', '-1.0']\n",
      "['-2.0', '-2.0', '-1.0', '+0.0']\n",
      "policy\n",
      "['____', '__L_', '__L_', 'UDLR']\n",
      "['U___', 'U_L_', 'UDLR', '_D__']\n",
      "['U___', 'UDLR', '_D_R', '_D__']\n",
      "['UDLR', '___R', '___R', '____']\n",
      "\n",
      "step:  3\n",
      "state values\n",
      "['+0.0', '-1.0', '-2.0', '-3.0']\n",
      "['-1.0', '-2.0', '-3.0', '-2.0']\n",
      "['-2.0', '-3.0', '-2.0', '-1.0']\n",
      "['-3.0', '-2.0', '-1.0', '+0.0']\n",
      "policy\n",
      "['____', '__L_', '__L_', '_DL_']\n",
      "['U___', 'U_L_', 'UDLR', '_D__']\n",
      "['U___', 'UDLR', '_D_R', '_D__']\n",
      "['U__R', '___R', '___R', '____']\n",
      "\n",
      "step:  4\n",
      "state values\n",
      "['+0.0', '-1.0', '-2.0', '-3.0']\n",
      "['-1.0', '-2.0', '-3.0', '-2.0']\n",
      "['-2.0', '-3.0', '-2.0', '-1.0']\n",
      "['-3.0', '-2.0', '-1.0', '+0.0']\n",
      "policy\n",
      "['____', '__L_', '__L_', '_DL_']\n",
      "['U___', 'U_L_', 'UDLR', '_D__']\n",
      "['U___', 'UDLR', '_D_R', '_D__']\n",
      "['U__R', '___R', '___R', '____']\n"
     ]
    }
   ],
   "source": [
    "policy_iteration.perform(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
